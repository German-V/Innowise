{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "trained-moses",
   "metadata": {},
   "source": [
    "import libraries(I provide all libs that I need when make this tasks, if you need some external import them here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "induced-african",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import max, avg, min\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-photographer",
   "metadata": {},
   "source": [
    "create local SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "stock-partnership",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/05 10:11:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"PySpark_Basics\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "juvenile-blame",
   "metadata": {},
   "source": [
    "read csv with inferschema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "computational-liverpool",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- work_year: string (nullable = true)\n",
      " |-- experience_level: string (nullable = true)\n",
      " |-- employment_type: string (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- salary_currency: string (nullable = true)\n",
      " |-- salary_in_usd: string (nullable = true)\n",
      " |-- employee_residence: string (nullable = true)\n",
      " |-- remote_ratio: string (nullable = true)\n",
      " |-- company_location: string (nullable = true)\n",
      " |-- company_size: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "    .option('header', 'true')\\\n",
    "    .option(\"interschema\", 'true')\\\n",
    "    .load('ds_salaries.csv')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-dominant",
   "metadata": {},
   "source": [
    "read csv one more time with the same code and you will see that it almostly don't take time, because info already in SparkSession and it will not read nothing\n",
    "from this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aging-neighborhood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- work_year: string (nullable = true)\n",
      " |-- experience_level: string (nullable = true)\n",
      " |-- employment_type: string (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- salary_currency: string (nullable = true)\n",
      " |-- salary_in_usd: string (nullable = true)\n",
      " |-- employee_residence: string (nullable = true)\n",
      " |-- remote_ratio: string (nullable = true)\n",
      " |-- company_location: string (nullable = true)\n",
      " |-- company_size: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "    .option('header', 'true')\\\n",
    "    .option(\"interschema\", 'true')\\\n",
    "    .load('ds_salaries.csv')\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-tomorrow",
   "metadata": {},
   "source": [
    "write schema of scv on screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "least-communications",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- work_year: string (nullable = true)\n",
      " |-- experience_level: string (nullable = true)\n",
      " |-- employment_type: string (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- salary_currency: string (nullable = true)\n",
      " |-- salary_in_usd: integer (nullable = true)\n",
      " |-- employee_residence: string (nullable = true)\n",
      " |-- remote_ratio: integer (nullable = true)\n",
      " |-- company_location: string (nullable = true)\n",
      " |-- company_size: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-brother",
   "metadata": {},
   "source": [
    "create schema of this scv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "progressive-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "StructField('_c0', IntegerType()),\n",
    "    StructField('work_year', IntegerType()),\n",
    "    StructField('experience_level', StringType()),\n",
    "    StructField('employment_type', StringType()),\n",
    "    StructField('job_title', StringType()),\n",
    "    StructField('salary', IntegerType()),\n",
    "    StructField('salary_currency', StringType()),\n",
    "    StructField('salary_in_usd', IntegerType()),\n",
    "    StructField('employee_residence', StringType()),\n",
    "    StructField('remote_ratio', IntegerType()),\n",
    "    StructField('company_location', StringType()),\n",
    "    StructField('company_size', StringType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-sauce",
   "metadata": {},
   "source": [
    "restart kernel without cleaning output and after restarting you need to initialize SparkSession, after initialize start execute only cells from cell with schema=\n",
    "=StructType.... \n",
    "To restart kernel click Kernel, Restart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-hospital",
   "metadata": {},
   "source": [
    "read ds_salaries with predefined schema and compare results from this cell and cell with inferSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "literary-plaintiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.format(\"csv\")\\\n",
    "    .option('header', 'true')\\\n",
    "    .option('schema', schema)\\\n",
    "    .load(\"ds_salaries.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-joint",
   "metadata": {},
   "source": [
    "this happens because read operation is lazy(transformation), but if you use inferschema it start to be action that will create Spark Job, because Spark need to loop throw all file to check datatypes for all columns and this can harm to your code(if we compare to parquet, it will also go to check data types, but parquet provide meta information, so Spark will not go throw all file, he will just read meta information, but csv don't provide such meta information). Also header make Spark to create one more Spark Job to check first line\n",
    "to define name of columns and remember to skeep it when reading. Actual reading start when you will use first action. More about Spark Jobs you will see in next topic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-assurance",
   "metadata": {},
   "source": [
    "write schema of scv on screen one more time and compare with previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "solid-infection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- work_year: string (nullable = true)\n",
      " |-- experience_level: string (nullable = true)\n",
      " |-- employment_type: string (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- salary: string (nullable = true)\n",
      " |-- salary_currency: string (nullable = true)\n",
      " |-- salary_in_usd: string (nullable = true)\n",
      " |-- employee_residence: string (nullable = true)\n",
      " |-- remote_ratio: string (nullable = true)\n",
      " |-- company_location: string (nullable = true)\n",
      " |-- company_size: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-water",
   "metadata": {},
   "source": [
    "now continue to work with one of the dataframes that you create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ae7fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnRenamed(\"_c0\", \"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-belgium",
   "metadata": {},
   "source": [
    "print data in dataframe using df.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "legendary-alarm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------------+---------------+--------------------+--------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "| id|work_year|experience_level|employment_type|           job_title|  salary|salary_currency|salary_in_usd|employee_residence|remote_ratio|company_location|company_size|\n",
      "+---+---------+----------------+---------------+--------------------+--------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "|  0|     2020|              MI|             FT|      Data Scientist|   70000|            EUR|        79833|                DE|           0|              DE|           L|\n",
      "|  1|     2020|              SE|             FT|Machine Learning ...|  260000|            USD|       260000|                JP|           0|              JP|           S|\n",
      "|  2|     2020|              SE|             FT|   Big Data Engineer|   85000|            GBP|       109024|                GB|          50|              GB|           M|\n",
      "|  3|     2020|              MI|             FT|Product Data Analyst|   20000|            USD|        20000|                HN|           0|              HN|           S|\n",
      "|  4|     2020|              SE|             FT|Machine Learning ...|  150000|            USD|       150000|                US|          50|              US|           L|\n",
      "|  5|     2020|              EN|             FT|        Data Analyst|   72000|            USD|        72000|                US|         100|              US|           L|\n",
      "|  6|     2020|              SE|             FT| Lead Data Scientist|  190000|            USD|       190000|                US|         100|              US|           S|\n",
      "|  7|     2020|              MI|             FT|      Data Scientist|11000000|            HUF|        35735|                HU|          50|              HU|           L|\n",
      "|  8|     2020|              MI|             FT|Business Data Ana...|  135000|            USD|       135000|                US|         100|              US|           L|\n",
      "|  9|     2020|              SE|             FT|  Lead Data Engineer|  125000|            USD|       125000|                NZ|          50|              NZ|           S|\n",
      "| 10|     2020|              EN|             FT|      Data Scientist|   45000|            EUR|        51321|                FR|           0|              FR|           S|\n",
      "| 11|     2020|              MI|             FT|      Data Scientist| 3000000|            INR|        40481|                IN|           0|              IN|           L|\n",
      "| 12|     2020|              EN|             FT|      Data Scientist|   35000|            EUR|        39916|                FR|           0|              FR|           M|\n",
      "| 13|     2020|              MI|             FT|   Lead Data Analyst|   87000|            USD|        87000|                US|         100|              US|           L|\n",
      "| 14|     2020|              MI|             FT|        Data Analyst|   85000|            USD|        85000|                US|         100|              US|           L|\n",
      "| 15|     2020|              MI|             FT|        Data Analyst|    8000|            USD|         8000|                PK|          50|              PK|           L|\n",
      "| 16|     2020|              EN|             FT|       Data Engineer| 4450000|            JPY|        41689|                JP|         100|              JP|           S|\n",
      "| 17|     2020|              SE|             FT|   Big Data Engineer|  100000|            EUR|       114047|                PL|         100|              GB|           S|\n",
      "| 18|     2020|              EN|             FT|Data Science Cons...|  423000|            INR|         5707|                IN|          50|              IN|           M|\n",
      "| 19|     2020|              MI|             FT|  Lead Data Engineer|   56000|            USD|        56000|                PT|         100|              US|           M|\n",
      "+---+---------+----------------+---------------+--------------------+--------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/05 10:11:55 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , work_year, experience_level, employment_type, job_title, salary, salary_currency, salary_in_usd, employee_residence, remote_ratio, company_location, company_size\n",
      " Schema: _c0, work_year, experience_level, employment_type, job_title, salary, salary_currency, salary_in_usd, employee_residence, remote_ratio, company_location, company_size\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/germanvaschelin/Desktop/Программы/Task4/PySpark_Basics/ds_salaries.csv\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca149184",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"salary_in_usd\", col(\"salary_in_usd\").cast(IntegerType()))\n",
    "df = df.withColumn(\"salary\", col(\"salary\").cast(IntegerType()))\n",
    "df = df.withColumn(\"remote_ratio\", col(\"remote_ratio\").cast(IntegerType()))\n",
    "df = df.withColumn(\"id\", col(\"id\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aab1997f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- work_year: string (nullable = true)\n",
      " |-- experience_level: string (nullable = true)\n",
      " |-- employment_type: string (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- salary_currency: string (nullable = true)\n",
      " |-- salary_in_usd: integer (nullable = true)\n",
      " |-- employee_residence: string (nullable = true)\n",
      " |-- remote_ratio: integer (nullable = true)\n",
      " |-- company_location: string (nullable = true)\n",
      " |-- company_size: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tamil-medium",
   "metadata": {},
   "source": [
    "print data in dataframe using display(df.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "connected-dryer",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'distutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Программы/Task4/venv/lib/python3.12/site-packages/pyspark/sql/pandas/conversion.py:86\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _create_converter_to_pandas\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m require_minimum_pandas_version\n\u001b[0;32m---> 86\u001b[0m \u001b[43mrequire_minimum_pandas_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     90\u001b[0m jconf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession\u001b[38;5;241m.\u001b[39m_jconf\n",
      "File \u001b[0;32m~/Desktop/Программы/Task4/venv/lib/python3.12/site-packages/pyspark/sql/pandas/utils.py:24\u001b[0m, in \u001b[0;36mrequire_minimum_pandas_version\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# TODO(HyukjinKwon): Relocate and deduplicate the version specification.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m minimum_pandas_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0.5\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LooseVersion\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'distutils'"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opponent-gazette",
   "metadata": {},
   "source": [
    "create df_job_title that consists from all job_titles without duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "friendly-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job_title = df.select(col(\"job_title\")).distinct()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-architecture",
   "metadata": {},
   "source": [
    "print all rows from df_job_titles without truncating jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "asian-edition",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|job_title                               |\n",
      "+----------------------------------------+\n",
      "|3D Computer Vision Researcher           |\n",
      "|Lead Data Engineer                      |\n",
      "|Head of Machine Learning                |\n",
      "|Data Specialist                         |\n",
      "|Data Analytics Lead                     |\n",
      "|Machine Learning Scientist              |\n",
      "|Lead Data Analyst                       |\n",
      "|Data Engineering Manager                |\n",
      "|Staff Data Scientist                    |\n",
      "|ETL Developer                           |\n",
      "|Director of Data Engineering            |\n",
      "|Product Data Analyst                    |\n",
      "|Principal Data Scientist                |\n",
      "|AI Scientist                            |\n",
      "|Director of Data Science                |\n",
      "|Machine Learning Engineer               |\n",
      "|Lead Data Scientist                     |\n",
      "|Machine Learning Infrastructure Engineer|\n",
      "|Data Science Engineer                   |\n",
      "|Machine Learning Manager                |\n",
      "|Research Scientist                      |\n",
      "|Head of Data                            |\n",
      "|Cloud Data Engineer                     |\n",
      "|Machine Learning Developer              |\n",
      "|Data Scientist                          |\n",
      "|Finance Data Analyst                    |\n",
      "|Data Analyst                            |\n",
      "|Data Analytics Engineer                 |\n",
      "|Data Science Consultant                 |\n",
      "|Principal Data Engineer                 |\n",
      "|Lead Machine Learning Engineer          |\n",
      "|ML Engineer                             |\n",
      "|Analytics Engineer                      |\n",
      "|Data Science Manager                    |\n",
      "|Business Data Analyst                   |\n",
      "|Principal Data Analyst                  |\n",
      "|Applied Machine Learning Scientist      |\n",
      "|Financial Data Analyst                  |\n",
      "|Data Analytics Manager                  |\n",
      "|Computer Vision Engineer                |\n",
      "|Computer Vision Software Engineer       |\n",
      "|Big Data Engineer                       |\n",
      "|Head of Data Science                    |\n",
      "|NLP Engineer                            |\n",
      "|Big Data Architect                      |\n",
      "|Applied Data Scientist                  |\n",
      "|Data Architect                          |\n",
      "|BI Data Analyst                         |\n",
      "|Marketing Data Analyst                  |\n",
      "|Data Engineer                           |\n",
      "+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_job_title.show(df_job_title.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-pharmacy",
   "metadata": {},
   "source": [
    "create  df_analytic that will consists from max, avg, min USD salaries for all job_titles using groupBy. name of fields is avg_salary, min_salary, max_salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "naval-roller",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analytic = df.groupby(\"job_title\")\\\n",
    "    .agg(max(\"salary_in_usd\").alias('max_salary'),\n",
    "         avg(\"salary_in_usd\").alias('avg_salary'),\n",
    "         min(\"salary_in_usd\").alias('min_salary'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "french-pledge",
   "metadata": {},
   "source": [
    "print all rows from df_analytic without trancating jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bacterial-depression",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------+------------------+----------+\n",
      "|job_title                               |max_salary|avg_salary        |min_salary|\n",
      "+----------------------------------------+----------+------------------+----------+\n",
      "|3D Computer Vision Researcher           |5409      |5409.0            |5409      |\n",
      "|Lead Data Engineer                      |276000    |139724.5          |56000     |\n",
      "|Head of Machine Learning                |79039     |79039.0           |79039     |\n",
      "|Data Specialist                         |165000    |165000.0          |165000    |\n",
      "|Data Analytics Lead                     |405000    |405000.0          |405000    |\n",
      "|Machine Learning Scientist              |260000    |158412.5          |12000     |\n",
      "|Lead Data Analyst                       |170000    |92203.0           |19609     |\n",
      "|Data Engineering Manager                |174000    |123227.2          |59303     |\n",
      "|Staff Data Scientist                    |105000    |105000.0          |105000    |\n",
      "|ETL Developer                           |54957     |54957.0           |54957     |\n",
      "|Director of Data Engineering            |200000    |156738.0          |113476    |\n",
      "|Product Data Analyst                    |20000     |13036.0           |6072      |\n",
      "|Principal Data Scientist                |416000    |215242.42857142858|148261    |\n",
      "|AI Scientist                            |200000    |66135.57142857143 |12000     |\n",
      "|Director of Data Science                |325000    |195074.0          |130026    |\n",
      "|Machine Learning Engineer               |250000    |104880.14634146342|20000     |\n",
      "|Lead Data Scientist                     |190000    |115190.0          |40570     |\n",
      "|Machine Learning Infrastructure Engineer|195000    |101145.0          |50180     |\n",
      "|Data Science Engineer                   |127221    |75803.33333333333 |40189     |\n",
      "|Machine Learning Manager                |117104    |117104.0          |117104    |\n",
      "|Research Scientist                      |450000    |109019.5          |42000     |\n",
      "|Head of Data                            |235000    |160162.6          |32974     |\n",
      "|Cloud Data Engineer                     |160000    |124647.0          |89294     |\n",
      "|Machine Learning Developer              |100000    |85860.66666666667 |78791     |\n",
      "|Data Scientist                          |412000    |108187.83216783217|2859      |\n",
      "|Finance Data Analyst                    |61896     |61896.0           |61896     |\n",
      "|Data Analyst                            |200000    |92893.06185567011 |6072      |\n",
      "|Data Analytics Engineer                 |110000    |64799.25          |20000     |\n",
      "|Data Science Consultant                 |103000    |69420.71428571429 |5707      |\n",
      "|Principal Data Engineer                 |600000    |328333.3333333333 |185000    |\n",
      "|Lead Machine Learning Engineer          |87932     |87932.0           |87932     |\n",
      "|ML Engineer                             |270000    |117504.0          |15966     |\n",
      "|Analytics Engineer                      |205300    |175000.0          |135000    |\n",
      "|Data Science Manager                    |241000    |158328.5          |54094     |\n",
      "|Business Data Analyst                   |135000    |76691.2           |18442     |\n",
      "|Principal Data Analyst                  |170000    |122500.0          |75000     |\n",
      "|Applied Machine Learning Scientist      |423000    |142068.75         |31875     |\n",
      "|Financial Data Analyst                  |450000    |275000.0          |100000    |\n",
      "|Data Analytics Manager                  |150260    |127134.28571428571|105400    |\n",
      "|Computer Vision Engineer                |125000    |44419.333333333336|10000     |\n",
      "|Computer Vision Software Engineer       |150000    |105248.66666666667|70000     |\n",
      "|Big Data Engineer                       |114047    |51974.0           |5882      |\n",
      "|Head of Data Science                    |224000    |146718.75         |85000     |\n",
      "|NLP Engineer                            |37236     |37236.0           |37236     |\n",
      "|Big Data Architect                      |99703     |99703.0           |99703     |\n",
      "|Applied Data Scientist                  |380000    |175655.0          |54238     |\n",
      "|Data Architect                          |266400    |177873.9090909091 |90700     |\n",
      "|BI Data Analyst                         |150000    |74755.16666666667 |9272      |\n",
      "|Marketing Data Analyst                  |88654     |88654.0           |88654     |\n",
      "|Data Engineer                           |324000    |112725.0          |4000      |\n",
      "+----------------------------------------+----------+------------------+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_analytic.show(df_analytic.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-color",
   "metadata": {},
   "source": [
    "now you need to add in df_analytic column row_id, that will show order of all job_titles depending on avg salary. they should be descending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "nearby-treasurer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analytic = df_analytic.withColumn('row_id', row_number()\\\n",
    "    .over(Window.orderBy(col('avg_salary').desc())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-catalog",
   "metadata": {},
   "source": [
    "print all data from df_analytic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "confirmed-monitoring",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/05 10:12:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/05 10:12:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/05 10:12:32 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/05 10:12:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/05 10:12:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/05 10:12:33 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+----------+------------------+----------+------+\n",
      "|job_title                               |max_salary|avg_salary        |min_salary|row_id|\n",
      "+----------------------------------------+----------+------------------+----------+------+\n",
      "|Data Analytics Lead                     |405000    |405000.0          |405000    |1     |\n",
      "|Principal Data Engineer                 |600000    |328333.3333333333 |185000    |2     |\n",
      "|Financial Data Analyst                  |450000    |275000.0          |100000    |3     |\n",
      "|Principal Data Scientist                |416000    |215242.42857142858|148261    |4     |\n",
      "|Director of Data Science                |325000    |195074.0          |130026    |5     |\n",
      "|Data Architect                          |266400    |177873.9090909091 |90700     |6     |\n",
      "|Applied Data Scientist                  |380000    |175655.0          |54238     |7     |\n",
      "|Analytics Engineer                      |205300    |175000.0          |135000    |8     |\n",
      "|Data Specialist                         |165000    |165000.0          |165000    |9     |\n",
      "|Head of Data                            |235000    |160162.6          |32974     |10    |\n",
      "|Machine Learning Scientist              |260000    |158412.5          |12000     |11    |\n",
      "|Data Science Manager                    |241000    |158328.5          |54094     |12    |\n",
      "|Director of Data Engineering            |200000    |156738.0          |113476    |13    |\n",
      "|Head of Data Science                    |224000    |146718.75         |85000     |14    |\n",
      "|Applied Machine Learning Scientist      |423000    |142068.75         |31875     |15    |\n",
      "|Lead Data Engineer                      |276000    |139724.5          |56000     |16    |\n",
      "|Data Analytics Manager                  |150260    |127134.28571428571|105400    |17    |\n",
      "|Cloud Data Engineer                     |160000    |124647.0          |89294     |18    |\n",
      "|Data Engineering Manager                |174000    |123227.2          |59303     |19    |\n",
      "|Principal Data Analyst                  |170000    |122500.0          |75000     |20    |\n",
      "|ML Engineer                             |270000    |117504.0          |15966     |21    |\n",
      "|Machine Learning Manager                |117104    |117104.0          |117104    |22    |\n",
      "|Lead Data Scientist                     |190000    |115190.0          |40570     |23    |\n",
      "|Data Engineer                           |324000    |112725.0          |4000      |24    |\n",
      "|Research Scientist                      |450000    |109019.5          |42000     |25    |\n",
      "|Data Scientist                          |412000    |108187.83216783217|2859      |26    |\n",
      "|Computer Vision Software Engineer       |150000    |105248.66666666667|70000     |27    |\n",
      "|Staff Data Scientist                    |105000    |105000.0          |105000    |28    |\n",
      "|Machine Learning Engineer               |250000    |104880.14634146342|20000     |29    |\n",
      "|Machine Learning Infrastructure Engineer|195000    |101145.0          |50180     |30    |\n",
      "|Big Data Architect                      |99703     |99703.0           |99703     |31    |\n",
      "|Data Analyst                            |200000    |92893.06185567011 |6072      |32    |\n",
      "|Lead Data Analyst                       |170000    |92203.0           |19609     |33    |\n",
      "|Marketing Data Analyst                  |88654     |88654.0           |88654     |34    |\n",
      "|Lead Machine Learning Engineer          |87932     |87932.0           |87932     |35    |\n",
      "|Machine Learning Developer              |100000    |85860.66666666667 |78791     |36    |\n",
      "|Head of Machine Learning                |79039     |79039.0           |79039     |37    |\n",
      "|Business Data Analyst                   |135000    |76691.2           |18442     |38    |\n",
      "|Data Science Engineer                   |127221    |75803.33333333333 |40189     |39    |\n",
      "|BI Data Analyst                         |150000    |74755.16666666667 |9272      |40    |\n",
      "|Data Science Consultant                 |103000    |69420.71428571429 |5707      |41    |\n",
      "|AI Scientist                            |200000    |66135.57142857143 |12000     |42    |\n",
      "|Data Analytics Engineer                 |110000    |64799.25          |20000     |43    |\n",
      "|Finance Data Analyst                    |61896     |61896.0           |61896     |44    |\n",
      "|ETL Developer                           |54957     |54957.0           |54957     |45    |\n",
      "|Big Data Engineer                       |114047    |51974.0           |5882      |46    |\n",
      "|Computer Vision Engineer                |125000    |44419.333333333336|10000     |47    |\n",
      "|NLP Engineer                            |37236     |37236.0           |37236     |48    |\n",
      "|Product Data Analyst                    |20000     |13036.0           |6072      |49    |\n",
      "|3D Computer Vision Researcher           |5409      |5409.0            |5409      |50    |\n",
      "+----------------------------------------+----------+------------------+----------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_analytic.show(df_analytic.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-quarter",
   "metadata": {},
   "source": [
    "it isn't beautifull, so we need to put now row_id on first place in df_analytic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ranging-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analytic = df_analytic.select(\"row_id\", \"job_title\", \"max_salary\", \"avg_salary\", \"min_salary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reliable-amsterdam",
   "metadata": {},
   "source": [
    "print df_analytic now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "classical-biology",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/05 10:12:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/05 10:12:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/05 10:12:42 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/05 10:12:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/05 10:12:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "24/01/05 10:12:43 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------------------------------+----------+------------------+----------+\n",
      "|row_id|job_title                               |max_salary|avg_salary        |min_salary|\n",
      "+------+----------------------------------------+----------+------------------+----------+\n",
      "|1     |Data Analytics Lead                     |405000    |405000.0          |405000    |\n",
      "|2     |Principal Data Engineer                 |600000    |328333.3333333333 |185000    |\n",
      "|3     |Financial Data Analyst                  |450000    |275000.0          |100000    |\n",
      "|4     |Principal Data Scientist                |416000    |215242.42857142858|148261    |\n",
      "|5     |Director of Data Science                |325000    |195074.0          |130026    |\n",
      "|6     |Data Architect                          |266400    |177873.9090909091 |90700     |\n",
      "|7     |Applied Data Scientist                  |380000    |175655.0          |54238     |\n",
      "|8     |Analytics Engineer                      |205300    |175000.0          |135000    |\n",
      "|9     |Data Specialist                         |165000    |165000.0          |165000    |\n",
      "|10    |Head of Data                            |235000    |160162.6          |32974     |\n",
      "|11    |Machine Learning Scientist              |260000    |158412.5          |12000     |\n",
      "|12    |Data Science Manager                    |241000    |158328.5          |54094     |\n",
      "|13    |Director of Data Engineering            |200000    |156738.0          |113476    |\n",
      "|14    |Head of Data Science                    |224000    |146718.75         |85000     |\n",
      "|15    |Applied Machine Learning Scientist      |423000    |142068.75         |31875     |\n",
      "|16    |Lead Data Engineer                      |276000    |139724.5          |56000     |\n",
      "|17    |Data Analytics Manager                  |150260    |127134.28571428571|105400    |\n",
      "|18    |Cloud Data Engineer                     |160000    |124647.0          |89294     |\n",
      "|19    |Data Engineering Manager                |174000    |123227.2          |59303     |\n",
      "|20    |Principal Data Analyst                  |170000    |122500.0          |75000     |\n",
      "|21    |ML Engineer                             |270000    |117504.0          |15966     |\n",
      "|22    |Machine Learning Manager                |117104    |117104.0          |117104    |\n",
      "|23    |Lead Data Scientist                     |190000    |115190.0          |40570     |\n",
      "|24    |Data Engineer                           |324000    |112725.0          |4000      |\n",
      "|25    |Research Scientist                      |450000    |109019.5          |42000     |\n",
      "|26    |Data Scientist                          |412000    |108187.83216783217|2859      |\n",
      "|27    |Computer Vision Software Engineer       |150000    |105248.66666666667|70000     |\n",
      "|28    |Staff Data Scientist                    |105000    |105000.0          |105000    |\n",
      "|29    |Machine Learning Engineer               |250000    |104880.14634146342|20000     |\n",
      "|30    |Machine Learning Infrastructure Engineer|195000    |101145.0          |50180     |\n",
      "|31    |Big Data Architect                      |99703     |99703.0           |99703     |\n",
      "|32    |Data Analyst                            |200000    |92893.06185567011 |6072      |\n",
      "|33    |Lead Data Analyst                       |170000    |92203.0           |19609     |\n",
      "|34    |Marketing Data Analyst                  |88654     |88654.0           |88654     |\n",
      "|35    |Lead Machine Learning Engineer          |87932     |87932.0           |87932     |\n",
      "|36    |Machine Learning Developer              |100000    |85860.66666666667 |78791     |\n",
      "|37    |Head of Machine Learning                |79039     |79039.0           |79039     |\n",
      "|38    |Business Data Analyst                   |135000    |76691.2           |18442     |\n",
      "|39    |Data Science Engineer                   |127221    |75803.33333333333 |40189     |\n",
      "|40    |BI Data Analyst                         |150000    |74755.16666666667 |9272      |\n",
      "|41    |Data Science Consultant                 |103000    |69420.71428571429 |5707      |\n",
      "|42    |AI Scientist                            |200000    |66135.57142857143 |12000     |\n",
      "|43    |Data Analytics Engineer                 |110000    |64799.25          |20000     |\n",
      "|44    |Finance Data Analyst                    |61896     |61896.0           |61896     |\n",
      "|45    |ETL Developer                           |54957     |54957.0           |54957     |\n",
      "|46    |Big Data Engineer                       |114047    |51974.0           |5882      |\n",
      "|47    |Computer Vision Engineer                |125000    |44419.333333333336|10000     |\n",
      "|48    |NLP Engineer                            |37236     |37236.0           |37236     |\n",
      "|49    |Product Data Analyst                    |20000     |13036.0           |6072      |\n",
      "|50    |3D Computer Vision Researcher           |5409      |5409.0            |5409      |\n",
      "+------+----------------------------------------+----------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_analytic.show(df_analytic.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hybrid-queensland",
   "metadata": {},
   "source": [
    "here you need to create df_exp_lvl with the biggest usd_salary(biggest_salary) for each experience_level(you need to save all fields like in entire dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dental-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_exp_lvl = df.groupBy(\"experience_level\").agg(max(\"salary_in_usd\").alias(\"biggest_salary\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-hierarchy",
   "metadata": {},
   "source": [
    "print here df_exp_lvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a783477b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- work_year: string (nullable = true)\n",
      " |-- experience_level: string (nullable = true)\n",
      " |-- employment_type: string (nullable = true)\n",
      " |-- job_title: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      " |-- salary_currency: string (nullable = true)\n",
      " |-- salary_in_usd: integer (nullable = true)\n",
      " |-- employee_residence: string (nullable = true)\n",
      " |-- remote_ratio: integer (nullable = true)\n",
      " |-- company_location: string (nullable = true)\n",
      " |-- company_size: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "standing-toilet",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 48:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+\n",
      "|experience_level|biggest_salary|\n",
      "+----------------+--------------+\n",
      "|EX              |600000        |\n",
      "|MI              |450000        |\n",
      "|EN              |250000        |\n",
      "|SE              |412000        |\n",
      "+----------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_exp_lvl.show(df_exp_lvl.count(), truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-mortgage",
   "metadata": {},
   "source": [
    "create df_best that consists from rows where salary of guy same as biggest salary for other people in his exp_lvl and choose only columns: id, experience_level, biggest_salary, employee_residence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "toxic-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best = df.alias(\"a\").join(df_exp_lvl.alias(\"b\"), \n",
    "                  [df.salary_in_usd == df_exp_lvl.biggest_salary, \n",
    "                   df.experience_level == df_exp_lvl.experience_level],\n",
    "                  'inner')\\\n",
    "                      .select(col(\"a.id\"), col(\"a.experience_level\"), col(\"b.biggest_salary\"), col(\"a.employee_residence\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complex-librarian",
   "metadata": {},
   "source": [
    "print df_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "smart-texas",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+--------------+------------------+\n",
      "| id|experience_level|biggest_salary|employee_residence|\n",
      "+---+----------------+--------------+------------------+\n",
      "| 33|              MI|        450000|                US|\n",
      "| 37|              EN|        250000|                US|\n",
      "| 63|              SE|        412000|                US|\n",
      "| 97|              MI|        450000|                US|\n",
      "|252|              EX|        600000|                US|\n",
      "+---+----------------+--------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/05 10:13:07 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , experience_level, salary_in_usd, employee_residence\n",
      " Schema: _c0, experience_level, salary_in_usd, employee_residence\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/germanvaschelin/Desktop/Программы/Task4/PySpark_Basics/ds_salaries.csv\n"
     ]
    }
   ],
   "source": [
    "df_best.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-brass",
   "metadata": {},
   "source": [
    "drop duplicates if exist by experience_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "immune-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best = df_best.dropDuplicates(['experience_level'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-credit",
   "metadata": {},
   "source": [
    "print df_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "specified-wellington",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/05 10:13:13 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , experience_level, salary_in_usd, employee_residence\n",
      " Schema: _c0, experience_level, salary_in_usd, employee_residence\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/germanvaschelin/Desktop/Программы/Task4/PySpark_Basics/ds_salaries.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+--------------+------------------+\n",
      "| id|experience_level|biggest_salary|employee_residence|\n",
      "+---+----------------+--------------+------------------+\n",
      "| 37|              EN|        250000|                US|\n",
      "|252|              EX|        600000|                US|\n",
      "| 33|              MI|        450000|                US|\n",
      "| 63|              SE|        412000|                US|\n",
      "+---+----------------+--------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_best.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-plant",
   "metadata": {},
   "source": [
    "create df_new_best from df_best without id, and make the next: when exp_level = MI we want middle, when SE we want senior, else Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "infinite-retail",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_best = df_best.select('experience_level', 'biggest_salary', 'employee_residence',\n",
    "                            when(col('experience_level') == 'MI', 'Middle')\\\n",
    "                            .when(col('experience_level') == 'SE', 'Senior')\\\n",
    "                            .otherwise('Null').alias('new_experience_level'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "changing-fairy",
   "metadata": {},
   "source": [
    "print df_new_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "endless-framework",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+------------------+--------------------+\n",
      "|experience_level|biggest_salary|employee_residence|new_experience_level|\n",
      "+----------------+--------------+------------------+--------------------+\n",
      "|              EN|        250000|                US|                Null|\n",
      "|              EX|        600000|                US|                Null|\n",
      "|              MI|        450000|                US|              Middle|\n",
      "|              SE|        412000|                US|              Senior|\n",
      "+----------------+--------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new_best.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-status",
   "metadata": {},
   "source": [
    "write df_new_best like 1.csv and load then it to df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "baking-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_best.write.mode('overwrite').options(header = True, delimiter = ',').csv('1.csv')\n",
    "df_final = spark.read.format('csv')\\\n",
    "    .options(header = True, interschema = True)\\\n",
    "    .load('1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-shooting",
   "metadata": {},
   "source": [
    "print df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "expired-viewer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+------------------+--------------------+\n",
      "|experience_level|biggest_salary|employee_residence|new_experience_level|\n",
      "+----------------+--------------+------------------+--------------------+\n",
      "|              EN|        250000|                US|                Null|\n",
      "|              EX|        600000|                US|                Null|\n",
      "|              MI|        450000|                US|              Middle|\n",
      "|              SE|        412000|                US|              Senior|\n",
      "+----------------+--------------+------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-progress",
   "metadata": {},
   "source": [
    "filter df_final to delete experience_level where it Null, then join this table by biggest_salary(salary_in_usd) and employee_residence with entire df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "small-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.filter(col('new_experience_level')!='Null')\\\n",
    "    .join(df, \n",
    "        [df.salary_in_usd == df_final.biggest_salary, df.employee_residence == df_final.employee_residence])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-twins",
   "metadata": {},
   "source": [
    "print df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "generic-block",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------+------------------+--------------------+---+---------+----------------+---------------+----------------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "|experience_level|biggest_salary|employee_residence|new_experience_level|id |work_year|experience_level|employment_type|job_title             |salary|salary_currency|salary_in_usd|employee_residence|remote_ratio|company_location|company_size|\n",
      "+----------------+--------------+------------------+--------------------+---+---------+----------------+---------------+----------------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "|MI              |450000        |US                |Middle              |33 |2020     |MI              |FT             |Research Scientist    |450000|USD            |450000       |US                |0           |US              |M           |\n",
      "|SE              |412000        |US                |Senior              |63 |2020     |SE              |FT             |Data Scientist        |412000|USD            |412000       |US                |100         |US              |L           |\n",
      "|MI              |450000        |US                |Middle              |97 |2021     |MI              |FT             |Financial Data Analyst|450000|USD            |450000       |US                |100         |US              |L           |\n",
      "+----------------+--------------+------------------+--------------------+---+---------+----------------+---------------+----------------------+------+---------------+-------------+------------------+------------+----------------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/05 10:13:25 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: , work_year, experience_level, employment_type, job_title, salary, salary_currency, salary_in_usd, employee_residence, remote_ratio, company_location, company_size\n",
      " Schema: _c0, work_year, experience_level, employment_type, job_title, salary, salary_currency, salary_in_usd, employee_residence, remote_ratio, company_location, company_size\n",
      "Expected: _c0 but found: \n",
      "CSV file: file:///Users/germanvaschelin/Desktop/Программы/Task4/PySpark_Basics/ds_salaries.csv\n"
     ]
    }
   ],
   "source": [
    "df_final.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-moore",
   "metadata": {},
   "source": [
    "last task is to save in variable and then print this variable of the biggest salary_in_usd from df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "individual-institution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|biggest|\n",
      "+-------+\n",
      "| 450000|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "variable = df_final.select('biggest_salary','salary_in_usd').agg(max(col('salary_in_usd')).alias('biggest')).drop('salary_in_usd')\n",
    "variable.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stopped-procedure",
   "metadata": {},
   "source": [
    "It is the end of PySpark basics. In other lessons you will learn optimizations technics and how to make distributed system"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
